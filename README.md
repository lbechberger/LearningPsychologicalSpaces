# LearningPsychologicalSpaces
v0.1: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1220053.svg)](https://doi.org/10.5281/zenodo.1220053)
v1.1: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3340766.svg)](https://doi.org/10.5281/zenodo.3340766)

The code in this repository explores learning a mapping from images to psychological similarity spaces with neural networks. 

Research based on an earlier version (v0.1) of this repository has been presented at [AIC 2018](http://aic2018.pa.icar.cnr.it/): 

Lucas Bechberger and Elektra Kypridemou. "Mapping Images to Psychological Similarity Spaces Using Neural Networks" [Paper](http://ceur-ws.org/Vol-2418/) [Preprint](https://arxiv.org/abs/1804.07758)

## 1 About

Our scripts use TensorFlow 1.10 with Python 3.5 along with scikit-learn. You can find scripts for setting up a virtual environment with anaconda in the [Utilities](https://github.com/lbechberger/Utilities) project.

The folder `code` contains all python scripts used in our experiments. The usage of these scripts is detailed below.
The folder `data` contains the data used for the NOUN study inside the `NOUN` subfolder. This includes the dissimilarity ratings, the images, as well as all intermediate products created by our scripts. In the subfolder `Shapes` we will at some point add the respective results for the Shape study.

Both studies consist of two parts: The first part focuses on the spaces produced by multidimensional scaling (to be found in the `mds` subfolders of both `code` and `data`), whereas the second part focuses on learning a mapping from images into these similarity spaces (to be found in the `ml` subfolders of both `code` and `data`).

Our training data are the images and similarity ratings of the NOUN database (http://www.sussex.ac.uk/wordlab/noun), which were kindly provided by Jessica Horst and Michael Hout: 
Horst, Jessica S., and Michael C. Hout. "The Novel Object and Unusual Name (NOUN) Database: A collection of novel images for use in experimental research." Behavior research methods 48.4 (2016): 1393-1409.

When executing the python scripts, we recommend to execute them as modules (`python -m path.to.module`) - this is necessary for some scripts that make use of utility functions stored in the module `code/util.py`.

### 1.1 The NOUN study

For the study on the NOUN data set, we compared the spaces obtainable by four MDS algorithms (classical MDS, Kruskal's MDS algorithm, metric SMACOF, and nonmetric SMACOF). We investigated both metric and nonmetric stress. Moreover, we computed the correlations between the pairwise distances in the MDS spaces and the dissimilarity ratings. For the latter analysis, we also compared to a pixel-based and an ANN-based baseline.

In a second part of the study, we then trained regressors from either downsampled images or ANN activation to the similarity spaces, investigating a linear regression, a random forest regression, and a lasso regression. We compared the results obtainable on spaces of different sizes and spaces generated by different MDS algorithms.

The script `code/shell_scripts/pipeline_NOUN.py` automatically executes all scripts necessary to reproduce our results. It requires one argument which can either be `paper` (in order to reproduce the results from our paper) or `dissertation` (in order to reproduce a more comprehensive set of results). The script then sets up some shell variables accordingly and calls the following five shell scripts which make up the five processing steps in our setup:
- `code/shell_scripts/NOUN/mds.sh`: Preprocesses the dissimilarity ratings (stored in `data/NOUN/mds/similarities/`), applies the different MDS algorithms for target spaces of different sizes (resulting vectors are stored in `data/NOUN/mds/vectors`), and visualizes the spaces with 2D plots (stored in `data/NOUN/mds/visualizations/spaces/`)
- `code/shell_scripts/NOUN/correlation.sh`: Computes different correlation metrics between the pairwise distances between items in the MDS spaces and the original dissimilarity ratings. Also computes the results for a pixel-based and an ANN-based baseline. All of the resulting values are stored in `data/NOUN/mds/correlations/`. Creates some line graphs illustrating how correlation develops based on the dimensionality of the MDS space, and the block size, respectively. These visualizations are stored in `data/NOUN/mds/visualizations/correlations/`.
- `code/shell_scripts/NOUN/ml_setup.sh`: Creates a machine learning data set by appyling data augmentation to the original images, by preparing the target vectors in the similarity space, and by extracting ANN-based and pixel-based features. The resulting data set is stored in multiple files in `data/NOUN/ml/dataset/`.
- `code/shell_scripts/NOUN/experiment_1.sh`: Executes the first machine learning experiment, where we analyze the performance of different regressors (linear regression, random forest regression, lasso regression) on different feature sets (ANN-based vs. pixel-based), using a fixed target space. The results are stored in `data/NOUN/ml/experiment_1/`.
- `code/shell_scripts/NOUN/experiment_2.sh`: Executes the second machine learning experiment, where we analyze the performance on target spaces of the same size that have been created by different MDS algorithms. The results are stored in `data/NOUN/ml/experiment_2/`.
- `code/shell_scripts/NOUN/experiment_3.sh`: Executes the second machine learning experiment, where we analyze the performance on target spaces of different dimensionality that have been created by a single MDS algorithm. The results are stored in `data/NOUN/ml/experiment_3/`.

The only files necessary to run all of these experiments are `data/NOUN/mds/raw_data/raw_distances.csv` (the original dissimilarity matrix from Horst and Hout's NOUN study), `data/NOUN/mds/raw_data/4D-vectors.csv` (the vectors of their four-dimensional similarity space), `data/NOUN/ml/targets.csv` (defining which similarity spaces are included as possible targets in the machine learning data set), and `data/NOUN/ml/folds.csv` (defining the structure of the folds for the cross validation). If the script `code/shell_scripts/clean_NOUN.sh` is executed, all files and folders except for the ones listed above are deleted.


### 1.2 The Shapes study

**Future work**

## 2 Multidimensional Scaling

The folder `code/mds` contains all scripts necessary for the first part of our studies, where we apply multidimensional scaling to a matrix of dissimilarity ratings and where we analyze the resulting spaces.

### 2.1 Preprocessing

The folder `code/mds/preprocessing` contains various scripts for preprocessing the data set in order to prepare it for multidimensional scaling. Depending on the data set (NOUN vs. Shapes), the first preprocessing step differs (see below), but results in the same output data structure, namely a dictionary with the following elements:
- `'categories'`: A dictionary using category names as keys and containing dictionaries as values. These dictionaries have the following elements:
  - `'visSim'`: Is the category visually homogeneous? 'Sim' means homogeneous, 'Dis' means not homogeneous, and 'x' means unclear. 
  - `'artificial'`: Does the category consist of natural ('nat') or artificial ('art') items? 
  - `'items'`: A list of the IDs of all items that belong into this category. 
- `'items'`: A dictionary using item IDs as keys and containing dictionaries as values. These dictionaries have the following elements:
  - `'name'`: A human readable name for the item. 
  - `'category'`: The name of the category this item belongs to. 
- `'similarities'`: A dictionary using the string representation of sets of two items as keys and dictionaries as values. These dictionaries have the following elements:
  - `'relation'`: Is this a 'within' category or a 'between' category rating? 
  - `'values'`: A list of similarity values (integers from 1 to 5, where 1 means 'no visual similarity' and 5 means 'high visual similarity')
  - `'border'`: An integer indicating the border between similarity ratings from the two studies. You can use `values[:border]` to access only the similarity ratings of the first study (only within category) and `values[border:]` to acces only the similarity ratings of the second study (mostly between cateory, but also some within category). **


#### 2.1.1 Parsing NOUN Similarity Data

The script `preprocess_NOUN.py` reads in the distances obtained via SpAM and stores them in a pickle file for further processing. It can be executed as follows from the project's root directory:
```
python -m code.mds.preprocessing.preprocess_NOUN path/to/distance_table.csv path/to/output.pickle
```
The script reads the distance information from the specified csv file and stores it together with some further information in the specified pickle file. 

With respect to the resulting `output.pickle` file, we would like to make the following comments: As the NOUN data set does not divide the stimuli into separate categories, we store all items under a single global category which is considered to consist of artificial stimuli. We assume that the visual homogeneity of this category is unclear. As the individual stimuli do not have meaningful names, we use their IDs also as human readable names. Finally, as there is only a single study, we set `border` to zero.

#### 2.1.2 Parsing Shapes Similarity Data

In order to make the Shapes data processible by our scripts, please run the script `preprocess_Shapes.py` as follows from the project's root directory:
```
python -m code.mds.preprocessing.preprocess_Shape path/to/within.csv path/to/within_between.csv path/to/output.pickle
```

The file `within.csv` contains within category similarity judments (first study), the file `within_between.csv` contains similarity ratings both within and between categories (second study). The optional flag `-r` or `--reverse` can be set in order to reverse the order of similarity ratings (necessary when using conceptual similarity, as the scale there is inverted).

The resulting `output.pickle` file makes use of the full dictionary structure outlined above.

#### 2.1.3 Parsing Shape Features Data

The script `preprocess_feature.py` reads in the feature ratings (both pre-attentive and attentive) from two given CSV files and stores them in a single pickle file for further usage. It can be invoked as follows:
```
python -m code.mds.preprocessing.preprocess_feature path/to/pre_attentive_ratings.csv path/to/attentive_ratings.csv path/to/output.pickle
```

The resulting `output.pickle` file contains a single dictionary which maps each item ID to a dictionary with the following entries:
- `'pre-attentive'`: A list of tuples consisting of the respective reaction time and the respective response (True/False/None).
- `'attentive'`: A list of continuous scale ratings.
- `'name'`: A human readable name for the item.

The script takes the following optional arguments:
- `-p`or `--plot`: If this flag is set, histograms for the response times and the continuous values are created.
- `-o` or `--output`: Path to the folder in which the histograms shall be stored.

#### 2.1.4 Aggregating Similarity Ratings

The next step in the preprocessing pipeline is to extract similarity ratings from the overall data set. This can be done with the script `compute_similarities.py`. You can execute it as follows from the project's root directory:
```
python -m code.preprocessing.compute_similarities path/to/input_file.pickle path/to/output_file.pickle
```

The first argument to this script should be the output file generated by any of the two preprocessing scripts, the second parameter determines where the resulting similarity values are stored. After converting the data from the input pickle file into a dissimilarity matrix, some information about this matrix is printed out.

The script takes the following optional arguments:
- `-s` or `--subset`: Specifies which subset of the similarity ratings to use. Default is `all` (which means that all similarity ratings from both studies are used). Another supported option is `between` where only the ratings from the second study (i.e., `values[border:]`) are used. Here, all items that did not appear in the second study are removed from the dissimilarity matrix. A third option is `cats` which only considers the categories used in the second study, but which keeps all items from these categories (also items that were only used in the first, but not in the second study). The fourth option `within` only uses data from the first study (i.e., `values[:border]`).
- `-m` or `--median`: Use the median instead of the mean for aggregating the similarity ratings across participants.
- `-l` or `--limit`: Limit the number of similarity ratings to use to ensure that an equal amount of ratings is aggregated for all item pairs. 
- `-v` or `--limit_value`: Used to give an explicit value for the limit to use. If not set, the script will use the minimal number of ratings observed for any item pair as limit.
- `-p` or `--plot`: Plot some histograms of the similarity values and store them in the same folder where also the output pickle file is stored.

The result is a pickle file which consists of a dictionary with the following content:
- `'items'`: The list of item-IDs of all the items for which the similarity values have been computed
- `'item_names'`: The list of item names for all the items (sorted in same way as `'items'`).
- `'similarities'`: A quadratic matrix of similarity values. Both rows and columns are ordered like in `'items'`. Values of `nan` are used to indicate that there is no similarity rating available for a pair of stimuli.
- `'dissimilarities'`: A quadratic matrix of dissimilarity values analogous to `'similarities'`. Here, values of 0 indicate missing similarity ratings.

#### 2.1.5 Analyzing the Distribution of Similarity Ratings

The script `analyze_similarity_distribution.py` can be used to collect some statistics on the distribution of similarity ratings for a given subset of the data with respect to the "Sim"-"Dis" category distinction. It can be executed as follows:
```
python -m code.mds.preprocessing.analyze_similarity_distribution path/to/input_file.pickle
```
The input file is here the `output.pickle` created by the `preprocess_Shapes.py` script. The script takes the following optional parameters:

- `-s` or `--subset`: Specifies which subset of the similarity ratings to use. Default is `all` (which means that all similarity ratings from both studies are used). Other supported options are `between`, `within`,  and `cats` (see above).
- `-m` or `--median`: Use the median instead of the mean for aggregating the similarity ratings across participants.
- `-l` or `--limit`: Limit the number of similarity ratings to use to ensure that an equal amount of ratings is aggregated for all item pairs. 
- `-v` or `--limit_value`: Used to give an explicit value for the limit to use. If not set, the script will use the minimal number of ratings observed for any item pair as limit.

#### 2.1.6 Creating Average Images

The script `average_images.py` can be used in order to create an average image for each of the categories. It can be invoked as follows:
```
python -m code.mds.preprocessing.average_images path/to/input_file.pickle path/to/image_folder
```
Here, `input_file.pickle` corresponds to the output file of `preprocess_Shapes.py` or `preprocess_NOUN.py` and `image_folder` points to the folder where all the original images reside. The script takes the following optional arguments:
- `-o` or `--output_folder`: The destination folder for the output images, defaults to `.`, i.e., the current working directory.
- `-r` or `--resolution`: The desired size (width and height) of the output images, defaults to 283 (i.e, the size of the original images from the Shapes data set).
- `-s` or `--subset`: The subset of data to use, defaults to `all`. Possible other options are `between`, `within`, and `cats` (see above).
- `-a` or `--aggregator`: The aggregator to use for downscaling the images. Defaults to `mean`. Other possible values are `min`, `max`, and `median`.

#### 2.1.7 Analyzing Ratings of Psychological Features

In order to analyze the ratings for the psychological features, we can use the script `analyze_feature.py` which can be called as follows:
```
python -m code.mds.preprocessing.analyze_feature path/to/input.pickle path/to/analysis_folder path/to/classification.pickle path/to/regression.pickle
```

Here, `input.pickle` is the file generated by `preprocess_feature.py`. The script analyzes the pre-attentive and attentive ratings for the given psychological feature and stores a scatter plot in the given `analysis_folder`. Information about the analysis results are printed to the console. 

Moreover, the script derives a classification structure for downstream processing, which is stored in the `classification.pickle` file as a dictionary with the following entries:
- `'pre-attentive'`: Classification information based on the pre-attentive classification responses.
- `'attentive'`: Classification information based on the attentive rating responses.

In each case, the classification information is stored as an inner dictionary with the keys `'positive'` and `'negative'` mapping to a list of item IDs belonging to the positive and negative classes, respectively.

Finally, the script also creates a regression structure for downstream processing, which is stored in `regression.pickle` as a dictionary with the following entries:
- `'pre-attentive'`: Regression information based on the pre-attentive classification responses.
- `'attentive'`: Regression information based on the attentive rating responses.

In each case, the regression information is stored as an inner dictionary mapping from item IDs to float values in [-1,1] representing their respective value on the regression target scale.

The script accepts the following optional parameters:
- `-i` or `--image_folder`: Path to the folder containing the images for the items. If given, it will use the item images to create scatter plots. If not given, an ordinary scatter plot will be used.
- `-z` or `--zoom`: Determines the size of the item images in the scatter plot. Defaults to 0.15.
- `-r` or `--response_times`: If this flag is set, the response times from the pre-attentive classification are also converted into classification and regression information. This creates one additional entry in each of the pickle files, namely `'rt'`.
- `-m` or `--median`: If this flag is set, the median is used for aggregation (both of attentive ratings and of response times) instead of the arithmetic mean.

#### 2.1.8 Correlations between Psychological Features

The script `compare_features.py` compares the scales of two different psychological features to each other, based on each of the scale types. It can be invoked as follows:
```
python -m code.mds.preprocessing.compare_features path/to/first.pickle path/to/second.pickle path/to/output_folder
```
The data about the first and second feature is read in from `first.pickle` and `second.pickle`, respectively. Both files are the `regression.pickle` output files of the `analyze_feature.py` script. Scatter plots of the two features are stored in the given `output_folder`. The script takes the following optional arguments:
- `-i` or `--image_folder`: Path to the folder containing the images for the items. If given, it will use the item images to create scatter plots. If not given, an ordinary scatter plot will be used.
- `-z` or `--zoom`: Determines the size of the item images in the scatter plot. Defaults to 0.15.
- `-f` or `--first_name`: Name to use for the first feature. Defaults to `first`.
- `-s` or `--second_name`: Name to use for the second feature. Defaults to `second`.

#### 2.1.9 Creating Features from Category Information

The script `features_from_categories.py` uses the category structure to create candidate features based on both the `visSim` and the `artificial` information. It loads the raw data from `input.pickle` (output of `preprocess_Shapes.py`) and stores the regression and classification information in the given `regression_folder` and `classification_folder`, respectively. The script can be executed as follows:
```
python -m code.mds.preprocessing.features_from_categories path/to/input.pickle path/to/regression_folder path/to/classification_folder
```
The script accepts the following optional arguments:
- `-s` or `--subset`: The subset of data to use, defaults to `all`. Possible other options are `between`, `within`, and `cats` (see above).

#### 2.1.10 Writing CSV Files of Aggregated Dissimilarities
The R script for multidimensional scaling that we will use in the next step needs the aggregated dissimilarity data in form of a CSV file. The script `pickle_to_csv.py` stores the similaritiy ratings from `input_file.pickle` into a CSV file called `distance_matrix.csv` as well as the list of item names in a file called `item_names.csv`. Both output files are stored in the given `output_folder`. The `input_file.pickle` should be the file created by `compute_similarities.py`. The script can be invoked as follows:
```
python -m code.mds.preprocessing.pickle_to_csv path/to/input_file.pickle path/to/output_folder/
```

### 2.2 Multidimensional Scaling

The folder `code/mds/similarity_spaces` contains various scripts for transforming the given data set from pairwise similarity ratings into a conceptual space and for analyzing the resulting space.

#### 2.2.1 Applying MDS

The script `mds.r` runs four different versions of multidimensional scaling based on the implementations in R. More specifically, it uses the Eigenvalue-based classical MDS ([cmdscale](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cmdscale.html)), Kruskal's nonmetric MDS ([isoMDS](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/isoMDS.html)), and both metric and nonmetric SMACOF ([smacofSym](https://cran.r-project.org/web/packages/smacof/smacof.pdf#page.55)). For Kruskal's algorithm and for SMACOF, multiple random starts are used and the best result is kept. You can execute the script as follows from the project's root directory:
```
Rscript code/mds/similarity_spaces/mds.r -d path/to/distance_matrix.csv -i path/to/item_names.csv -o path/to/output/directory
```
Here, `distance_matrix.csv` is a CSV file which contains the matrix of pairwise dissimilarities and `item_names.csv` contains the item names (one name per row, same order as in the distance matrix). These two files have ideally been created by `pickle_to_csv.py`. The resulting vectors are stored in the given output directory. All three of these arguments are mandatory. Moreover, a CSV file is created in the output directory, which stores bthe stress values (metric stress and three variants of nonmetric stress) for each of the generated spaces. 

In order to specify which MDS algorithm to use, one must set exactly one of the following flags: `--classical`, `--Kruskal`, `--metric_SMACOF`, `--nonmetric_SMACOF`.

The script takes the following optional arguments:
- `-k` or `--dims`: Specifies the maximal number of dimensions to investigate. Default value is 20, which means that the script will run the MDS algorithm 20 times, obtaining spaces of dimensionality 1 to 20.
- `-n` or `--n_init`: Specifies how often the nondeterministic MDS algorithms are restarted with a new random initialization. Of all of these runs, only the best result (i.e., the one with the lowest resulting stress) is kept. Default value here is 64.
- `m` or `--max_iter`: Specifies the maximum number of iterations computed within the nondeterministic MDS algorithms. Default values is 1000.
- `-s` or `--seed`: Specify a seed for the random number generator in order to make the results deterministic. If no seed is given, then the random number generator is not seeded.
- `-t` or `--tiebreaker`: Specifies the type of tie breaking used in the SMACOF algorithm (possible values: `primary`, `secondary`, `tertiary`, default: `primary`).

We implemented the MDS step in R and not in Python because R offers a greater variety of MDS algorithms. Moreover, nonmetric SMACOF with Python's `sklearn` library produced poor results which might be due to a programming bug.

#### 2.2.2 Normalizing the Similarity Spaces

In order to make the individual MDS solutions more comparable, we normalize them by moving their centroid to the origin and by making sure that their mean squared distance to the origin equals one. This is done by the script `normalize_spaces.py`, which can be invoked by simply giving it the path to the directory containing all the vector files:
```
python -m code.mds.similarity_spaces.normalize_spaces path/to/input_folder
```
The script **overrides** the original files. It can take the following optional arguments:
- `-b` or `--backup`: Create a backup of the original files. Will be stored in the same folder as the original files, file name is identical, but 'backup' is appended before the file extension.
- `-v` or `--verbose`: Prints some debug information during processing (old centroid, old root mean squared distance to origin, new centroid, new root mean squared distance to origin).

**It is important to run this script before using the MDS spaces for the machine learning task -- only by normalizing the spaces, we can make sure that the MSE values are comparable across spaces!**

#### 2.2.3 Visualizing the Similarity Spaces

The script `visualize_spaces.py` can be used to create two-dimensional plots of the similarity spaces. You can execute it as follows from the project's root directory:
```
python -m code.mds.similarity_spaces.visualize_spaces path/to/vector_folder path/to/output_folder/
```
The script reads in the vectors from all csv files in the `vector_folder`, creates two-dimensional plots for all pairs of dimensions, and stores them in the given `output_folder`. 

The script takes the following optional arguments:
- `-i` or `--image_folder`: Path to a folder where the images of the items are stored. If this is given, then the images from this folder are used in the visualization. If no image folder is given, then data points are labeled with their item ID.
- `-z` or `--zoom`: Determines how much the images are scaled. Default is 0.15.
- `-m` or `--max`: Determines the dimensionality of the largest space to be visualized. Defaults to 10.
- `-d` or `--directions_file`: If a path to a directions file (output of `filter_directions.py`) is given, then the given directions are also included into the plots.
- `-c` or `--criterion`: If directions are plotted, the given criterion decides which ones are used. Defaults to `kappa`. Can also be set to `spearman`.

#### 2.2.4 Checking for Overlap

The script `analyze_overlap.py` can be used to check whether the categories within the space are non-overlapping. *This is only applicable to the Shapes data set, as there are no categories in NOUN.*

The script iterates over all categories, builds a convex hull of the items belonging to this category and counts how many points from other categories lie within this convex hull. Each point that lies in the convex hull of a different concept is counted as one violation. The script outputs the number of violations for each category, together with an estimate of how many violations would be expected if points are randomly sampled from a uniform distribution, a normal distribution, or the overall set of given points.

The script finally outputs the total number of violations as well as the group-wise number of violations for visual similarity and for natrualness. For the latter two, all four possible combination of classes are analyzed. An output for the pair `Sim`-`Dis` gives the number of violations observed where items from a category which is visually dissimilar was found to lie within the convex hull of another category which is visually similar, i.e., the first element of the tuple gives the type of category used for building the convex hull, whereas the second element gives the type of category of the "intruder" items.

The script can be exectued as follows (where `n_dims` is the number of dimension of this specific similarity space):
```
python -m code.mds.similarity_spaces.analyze_overlap path/to/vectors.csv path/to/data.pickle n_dims
```
It takes the following optional arguments:
- `-o` or `--output_file`: If an output file is given, the results are appended to this file in CSV style.
- `-b` or `--baseline`: Ony if this flag is set, the script will also estimate the expected values of randomly drawn points.
- `-r` or `--repetitions`: Determines the number of repetitions used when sampling from the baselines. Defaults to 20. More samples means more accurate estimation, but longer runtime.
- `-s` or `--seed`: Specify a seed for the random number generator in order to make the results deterministic. If no seed is given, then the random number generator is not seeded.

#### 2.2.5 Analyzing Category Size

The script `analyze_concept_size.py` evaluates category size by computing the average distance to the category prototype for all categories. *This is only applicable to the Shapes data set, as there are no categories in NOUN.* The script can be invoked as follows:
```
python -m code.mds.similarity_spaces.analyze_concept_size path/to/vectors.csv path/to/data.pickle n_dims
```
Here, `vectors.csv` contains MDS vectors of dimensionality `n_dims` and `data.pickle` is the data set file created by `preprocess_Shapes.py`. The script takes the following optional parameters:
- `-o` or `--output_file`: If an output file is given, the results are appended to this file in CSV style.
- `-b` or `--baseline`: Ony if this flag is set, the script will also estimate the expected values of randomly drawn points.
- `-r` or `--repetitions`: Determines the number of repetitions used when sampling from the baselines. Defaults to 20. More samples means more accurate estimation, but longer runtime.
- `-s` or `--seed`: Specify a seed for the random number generator in order to make the results deterministic. If no seed is given, then the random number generator is not seeded.


#### 2.2.6 Searching for Interpretable Directions
The script `find_directions.py` tries to find interpretable directions in a given similarity space based on a regression or classification task. *This is only applicable to the Shapes data set, as there are no categories in NOUN.*
It can be invoked as follows (where `n_dims` is the number of dimensions of the underlying space):
```
python -m code.mds.similarity_spaces.find_directions path/to/vectors.csv n_dims path/to/classification.pickle path/to/regression.pickle path/to/output.csv
```
Here, `vectors.csv` contains the vectors in an `n_dims`-dimensional similarity space (produced by `mds.r`). Based on the classification and regression information (`classification.pickle` and `regression.pickle`, respectively - both outputs of `analyze_dimension.py`) the script constructs a classification and a regression problem and trains a linear SVM on them. The quality of the model fit is evaluated by extracting the normal vector of the separating hyperplane and by projecting all points onto this normal vector. Then, we use Cohen's kappa to measure how well a simple threshold classifier on the resulting values performs. Moreover, we compute the Spearman correlation of the projected vectors to the scale values from the regression problem. The resulting numbers are stored in `output.csv`, along with the extracted direction.

#### 2.2.7 Comparing Interpretable Directions
The script `compare_directions.py` compares the interpretable directions found by `find_directions.py` by using the cosine similarity. More specifically, the script iterates through all spaces with a dimenionality of maximally `n_dims`. For each space, it computes the average cosine similarity of all the interpretable directions for the same feature (which were however constructed based on different scales and different ML algorithms). Moreover, it computes the average cosine similarity for each pair of features (by comparing all pairs of directions). The results are stored in `output.csv`. The script furthermore requires an `input_folder`, which contains all csv files created by `find_directions.py` (and no additional files!).
The script can be executed as follows:
```
python -m code.mds.similarity_spaces.compare_directions path/to/input_folder/ n_dims path/to/output.csv
```

#### 2.2.8 Filtering Interpretable Directions
In order to aggregate the different candidate directions for each feature, one can use the script `filter_directions.py`. It can be invoked as follows:
```
python -m code.mds.similarity_spaces.filter_directions path/to/input_file.csv direction_name n_dims path/to/output.csv
```
The script loads the candidate directions along with their evaluation results from `input_file.csv` (which is the output of `find_directions.py`). For each space (up to `n_dims`), it compares all candidate directions based on Cohen's kappa and based on the Spearman correlation. For each of these evaluation metrics, the directions with the highest values are kept and averaged. The result (the dimensionality of the space, the `direction_name`, the averaged direction and the list of candidate directions it is based on) is written to `output.csv`.
The script takes the following optional parameters:
- `-k` or `--kappa_threshold`: Minimal value of Cohen's kappa required to pass the filter. Defaults to 0.
- `-s` or `--spearman_threshold`: Minimal value of the Spearman correlation required to pass the filter. Defaults to 0.

#### 2.2.9 Aggregating Evaluation Results for Interpretable Directions
In order to make the subsequent analysis easier, the script `aggregate_direction_results.py` can be used to aggregate the evaluation results created by `find_directions.py` based on direction name, scale type, and ML model (by averaging over the two other conditions). It is executed as follows, where `input_folder`contains the csv files created by `find_directions.py` (and no additional files!), `n_dims` is the maximal dimensionality of the similarity space to consider. The results will be stored as separate csv files inside the `output_folder`.
```
python -m code.mds.similarity_spaces.aggregate_direction_results path/to/input_folder/ n_dims path/to/output/folder
```

### 2.3 Correlations to Similarity Ratings

The folder `mds/correlations` contains scripts for estimating how well the MDS spaces represent the underlying similarity ratings. As a baseline, pixel-based similarities of the corresponding images as well as the similarities of an ANN's activation vectors are used. In all cases, we consider three distance measures (Euclidean, Manhattan, inner product) and five correlation metrics (Pearson's r, Spearman's rho, Kendall's tau, and the coefficient of determination R²).

#### 2.3.1 Pixel-Based Similarities

The script `pixel_correlations.py` loads the images and downscales them using scipy's `block_reduce` function (see [here](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.block_reduce)). Here, all pixels within a block of size `k` times `k` are aggregated via one of the following aggregation functions: maximum, minimum, arithmetic mean, median. The script automatically iterates over all possible combinations of `k` and the aggregation function.

The pixels of the resulting downscaled images are interpreted as one-dimensional feature vectors. All of the distance measures are used to build distance matrices, which are then in turn correlated with the original dissimilarity ratings (both using the raw distances and using a weighted version where dimension weights are estimated in a cross-validation). The script can be executed as follows, where `similarity_file.pickle` is the output file of the overall preprocessing and where `image_folder` is the directory containing all images:
```
python -m code.mds.correlations.pixel_correlations path/to/similarity_file.pickle path/to/image_folder
```

Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use in the analysis:
- `--pearson`: Compute Pearson's correlation coefficient (linear correlation).
- `--spearman`: Compute Spearman's correlation coefficient (monotone correlation).
- `--kendall`: Compute Kendall's correlation coefficient (monotone correlation).
- `--r2_linear`: Compute the coefficient of determination R² for a linear regression (linear correlation).
- `--r2_isotonic`: Compute the coefficient of determination R² for an isotonic regression (monotone correlation).

The script takes the following optional parameters:
- `-o` or `--output_file`: Path to the output csv file where the resulting correlation values are stored (default: `pixel.csv`).
- `-w` or `--width`: The width (and also height) of the full images, i.e., the maximal number of `k` to use (default: 300, i.e. the image size of the NOUN data set).
- `-g` or `--greyscale`: If this flag is set, the three color channels are collapsed into a single greyscale channel when loading the images. If not, full RGB information is used.
- `-n` or `--n_folds`: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5).
- `-s` or `--seed`: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded.

#### 2.3.2 ANN-Based Similarities

As a second baseline, we use the features extracted by the a neural network (more specifically, the [Inception-v3 network](https://arxiv.org/abs/1512.00567)) to predict the similarities between images from the data set. The corresponding script is called `ann_correlations.py` and is invoked as follows:
```
python -m code.mds.correlations.ann_correlations path/to/model_folder path/to/similarity_file.pickle path/to/image_folder
```
The script downloads the inception network into the given `model_folder`, takes all images from the `image_folder`, and computes the activation of the second-to-last layer of the ANN. This activation vector is then used as a feature vectors. All of the distance measures are used to build distance matrices, which are then in turn correlated with the original dissimilarity ratings from `similarity_file.pickle` (both using the raw distances and using a weighted version where dimension weights are estimated in a cross-validation). 

Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use in the analysis:
- `--pearson`: Compute Pearson's correlation coefficient (linear correlation).
- `--spearman`: Compute Spearman's correlation coefficient (monotone correlation).
- `--kendall`: Compute Kendall's correlation coefficient (monotone correlation).
- `--r2_linear`: Compute the coefficient of determination R² for a linear regression (linear correlation).
- `--r2_isotonic`: Compute the coefficient of determination R² for an isotonic regression (monotone correlation).

The script takes the following optional arguments:
- `-o` or `--output_file`: Path to the output csv file where the resulting correlation values are stored (default: `ann.csv`).
- `-n` or `--n_folds`: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5).
- `-s` or `--seed`: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded.

#### 2.3.3 MDS-Based Similarities

The script `mds_correlations.py` loads the MDS vectors and derives distances between pairs of stimuli based on the three distance measures. These distances are then correlated to the human dissimilarity ratings (both using the raw distances and using a weighted version where dimension weights are estimated in a cross-validation). The script can be executed as follows:
```
python -m code.mds.correlations.mds_correlations path/to/similarity_file.pickle path/to/mds_folder
```
Here, `similarity_file.pickle` is again the output file of the overall preprocessing, whereas `mds_folder` is the folder where the MDS vectors are stored. 

Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use in the analysis:
- `--pearson`: Compute Pearson's correlation coefficient (linear correlation).
- `--spearman`: Compute Spearman's correlation coefficient (monotone correlation).
- `--kendall`: Compute Kendall's correlation coefficient (monotone correlation).
- `--r2_linear`: Compute the coefficient of determination R² for a linear regression (linear correlation).
- `--r2_isotonic`: Compute the coefficient of determination R² for an isotonic regression (monotone correlation).

The script takes the following optional arguments:
- `-o` or `--output_file`: Path to the output csv file where the resulting correlation values are stored (default: `mds.csv`).
- `--n_min`: The size of the smallest space to investigate (defaults to 1).
- `--n_max`: The size of the largest space to investigate (defaults to 20).
- `-n` or `--n_folds`: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5).
- `-s` or `--seed`: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded.

#### 2.3.4 Similarities from Different Datasets/Aggregations

The script `similarity_correlations.py` compares the aggregated dissimilarity ratings from two pickle files (output of `compute_similarities.py`) by using the correlation metrics listed above. It can be invoked as follows:
```
python -m code.mds.correlations.similarity_correlations path/to/first_similarity_file.pickle path/to/second_similarity_file.pickle
```
The script accepts the following optional parameters:
- `-f` or `--first_name`: Descriptive name for the first set of dissimilarities. Used for output and plotting.
- `-s` or `--second_name`: Descriptive name for the second set of dissimilarities. Used for output and plotting.
- `-p` or `--plot`: If this flag is set, a scatter plot is created and stored.
- `-o` or `--output_folder`: Path to the folder where the scatter plot shall be saved. Defaults to `.`, i.e., the current working directory.
- `--sim_only`: Only consider items from categories based on visual similarity. If this argument is given, it needs to point to a pickle file produced by `preprocess_Shapes.py`.

#### 2.3.5. Feature-Based Similarities

If we interpret the values on the scales of the (psychological) features as coordinates of a similarity space, we can use these coordinates to also compute distances between stimuli. The script `feature_correlations.py` does exactly this and computes the correlation to the original dissimilarity ratings (both using the raw distances and using a weighted version where dimension weights are estimated in a cross-validation). It is called as follows:
```
python -m code.mds.correlations.feature_correlations path/to/similarity_file.pickle path/to/regression_folder
```
Here, `similarity_file.pickle` is again the output file of the overall preprocessing, whereas `regression_folder` is the folder where the scale information about the features is stored (i.e., the folder containing the `regression.pickle` output from `analyze_feature.py`. The script loads all pickle files from the `regression_folder` and looks at all possible combinations of features and scale type. 

Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use in the analysis:
- `--pearson`: Compute Pearson's correlation coefficient (linear correlation).
- `--spearman`: Compute Spearman's correlation coefficient (monotone correlation).
- `--kendall`: Compute Kendall's correlation coefficient (monotone correlation).
- `--r2_linear`: Compute the coefficient of determination R² for a linear regression (linear correlation).
- `--r2_isotonic`: Compute the coefficient of determination R² for an isotonic regression (monotone correlation).

The script takes the following optional arguments:
- `-o` or `--output_file`: Path to the output csv file where the resulting correlation values are stored (default: `features.csv`).
- `-n` or `--n_folds`: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5).
- `-s` or `--seed`: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded.


#### 2.3.6 Visualizing The Correlations

The script `visualize_correlations.py` can be used to visualize the results of both the pixel-based correlations and the MDS-based correlations as a function of block size and number of dimensions, respectively. It can be invoked as follows:
```
python -m code.mds.correlations.visualize_correlations path/to/pixel_file.csv path/to/mds_file.csv
```
Here, `pixel_file.csv` and `mds_file.csv` are the output files of `pixel_correlations.py` and `mds_correlations.py`, respectively. 

Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use for visualization:
- `--pearson`: Compute Pearson's correlation coefficient (linear correlation).
- `--spearman`: Compute Spearman's correlation coefficient (monotone correlation).
- `--kendall`: Compute Kendall's correlation coefficient (monotone correlation).
- `--r2_linear`: Compute the coefficient of determination R² for a linear regression (linear correlation).
- `--r2_isotonic`: Compute the coefficient of determination R² for an isotonic regression (monotone correlation).

The script takes the following additional optional arguments:
- `-o` or `--output`: The output folder where the resulting visualizations are stored (default: `.`, i.e., the current working directory).

#### 2.3.7 Creating Scatter Plots

For some further visualization, the script `scatter_plot.py` can be used in order to create a scatter plot of predicted distances versus actual dissimilarities. It is invoked as follows:
```
python -m code.mds.correlations.scatter_plot path/to/similarity_file.pickle path/to/output_image.png
```
Here, `similarity_file.pickle` refers to the file generated by the preprocessing and `output_image.png` is the file name under which the scatter plot will be stored. There are three different modes for the scatter plot generation (based on the three correlation approaches) and exactly one of them must be picked via an optional argument:
- `--mds` or `-m`: The given file of MDS vectors is used for computing the predicted distances.
- `--ann` or `-a`: The inception network is used for predicting distances, the given path determines where the pretrained network is stored.
	- `--image_folder` or `-i` gives the folder where all the images are stored. Defaults to `.`, i.e., the current working directory.
- `--pixel` or `-p`: The given aggregator is used to perform the image downscaling in order to obtain distances.
	- `--image_folder` or `-i` gives the folder where all the images are stored. Defaults to `.`, i.e., the current working directory.
	- `--block_size` or `-b` determines the block size (defaults to 1).
	- `--greyscale` or `-g`: If this flag is set, images are interpreted as greyscale.

For all three of these cases, the parameter `--distance` or `-d` determines which distance function to use (`Euclidean`, `Manhattan`, or `Cosine`).

In its basic version, the script uses fixed identical dimension weights of one. If the flag `--optimized` or `-o` is set, then optimal dimension weights are computed based on a cross-validation approach (identical to the scripts above). The following additional parameters control the behavior of this cross-validation:
- `-n` or `--n_folds`: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5).
- `-s` or `--seed`: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded.


## 3 Machine Learning

The folder `code/ml` contains all scripts necessary for the second part of our studies, where we apply machine learning techniques in order to learn a mapping from images to points in the similarity space.

### 3.1 Preparing the Data Set for Machine Learning

In order to run a regression from images to MDS coordinates, multiple preprocessing steps are necessary. Firstly, we need to augment our data set by creating a large amount of slightly distorted image variants. This is done in order to achieve a data set of reasonable size for a machine learning task. Moreover, for each of the images, the target MDS coordinates need to be prepared. All scripts for these steps can be found in the `code/ml/prprocessing` folder.

#### 3.1.1 Data Augmentation

We used [ImgAug](https://github.com/aleju/imgaug) for augmenting our image data set. This is done with the script `data_augmentation.py`. It can be invoked as follows:
```
python -m code.ml.preprocessing.data_augmentation path/to/image_folder/ path/to/output_folder/ n
```
The script searches for all jpg images in the given `image_folder`, creates `n` augmented samples of each image and stores the results in the given `output_folder` (one pickle file per original image). The script takes the following optional command line arguments:
- `-s` or `--seed`: Specify a seed for the random number generator in order to make the results deterministic. If no seed is given, then the random number generator is not seeded.
- `-i` or `--image_size`: The expected image size in pixels. Defaults to 300 (i.e., the image size of the NOUN data set).

Augmentation is done by appling the folloing operations in random order:
- *Horizontal flips*. The probability of a horizontal flip can be controlled with the optional parameter `--flip_prob` (defaults to 0.5).
- *Cropping*. The maximal relative amount of cropping for each side of the image can be controlled with `--crop_size` (default: 0.1).
- *Gaussian Blur*. The probability of using a Gaussian blur can be set via `--blur_prob` (default: 0.5) and it's sigma value via `--blur_sigma` (default: 0.5)
- *Varying the contrast*. The borders of possible contrast values (relative to the image's original contrast) can be set via `--contrast_min` (default: 0.75) and `--contrast_max` (default: 1.5).
- *Additive Gaussian noise*. The value of sigma is set via `--g_noise_sigma` (default: 0.05) and the probability of drawing a different value for each color channel independently by seeting `--g_noise_channel_prob` (default: 0.5)
- *Varying the brightness*. The borders of possible brightness values (relative to the image's original brightness) can be set via `--light_min` (default: 0.8) and `--light_max` (default: 1.2). Moreover, the probability of drawing a different value for each color channel independently is controlled by `--light_channel_prob` (default: 0.2)
- *Zooming*: The borders of possible zoom values (relative to the image's original size) can be set via `--scale_min` (default: 0.8) and `--scale_max` (default: 1.2).
- *Translation*: The relative amount of translation is set with `--translation` (default: 0.2)
- *Rotation*: The maximal rotation angle in degrees is controlled by `--rotation_angle` (default: 25).
- *Shearing*: The maximal shear angle in degrees is controlled by `--shear_angle` (default: 8).
- *Salt and pepper noise*: The amount of pixels to modify is set via `--sp_noise_prob` (default: 0.03)

#### 3.1.2 Visualizing Augmented Images

In order to visually check that the augmentation step worked, you can use the script `show_augmented_images.py` to display them. It can be executed as follows:
```
python -m code.ml.preprocessing.show_augmented_images path/to/augmented.pickle
```
Here, `augmented.pickle` is one of the pickle files created by `data_augmentation.py`. By default, the script displays three rows (adjustable via `-r` or `--rows`) and four columns (adjustable via `-c` or `--columns`).

#### 3.1.3 Defining Regression Targets

As our experiments are run against a wide variety of target spaces, we created a script called `prepare_targets.py` which for convenience collects all possible target vectors in a single pickle file. It moreover creates a shuffled version of the targets for later usage as a control case. The script can be invoked as follows:
```
python -m code.ml.preprocessing.prepare_targets path/to/input.csv path/to/output.pickle
```
Here, `input.csv` is a csv file with two columns: In each row, the first column contains a short descriptive name of the target space and the second column contains the path to the corresponding file with the MDS vectors (as created in Section 2.2.1 and normalized in Section 2.2.2). The script iterates through all these target spaces and collects the MDS vectors. When shuffling them, the same seed is used for all spaces to ensure that the results are comparable. By setting `-s` or `--seed`, the user can specify a fixed seed, otherwise a random seed is drawn in the beginning of the script. 

The result is stored in `output.pickle` as a dictionary having the names of the target spaces as keys and further dictionaries (with the keys `correct` and `shuffled` leading to dictionaries with the corresponding image-vector mappings) as values.


### 3.2 Linear Regression

As a first pass of the regression task, we evaluate some simple baselines (which disregard the images altogether) as well as some linear regressions based on either downscaled images or the features extracted by a pretrained neural network. All scripts are contained in the `code/ml/regression` folder.

#### 3.2.1 ANN-based Feature Extraction

In order to create feature vectors based on the activations of the [Inception-v3 network](https://arxiv.org/abs/1512.00567), one can use the script `ann_features.py`. It is invoked as follows:
```
python -m code.ml.regression.ann_features path/to/model_folder path/to/input_folder path/to/output.pickle
```
The script downloads the inception-v3 network into the folder specified by `model_folder`, reads all augmented images from the folder specified by `input_folder`, uses them as input to the inception network, grabs the activations of the second-to-last layer of the network (2048 neurons) and stores a dictionary mapping from image name to a list of feature vectors in the pickle file specified by `output.pickle`.

#### 3.2.2 Pixel-based Feature Extraction
In order to create feature vectors by downscaling the original images, one can use the script `pixel_features.py`. It is invoked as follows:
```
python -m code.ml.regression.pixel_features path/to/input_folder path/to/output.pickle
```
The script reads all augmented images from the folder specified by `input_folder`, downscales them them according to the way described already in Section 2.3.1, and stores a dictionary mapping from image name to a list of feature vectors in the pickle file specified by `output.pickle`. It takes the following optional arguments:
- `-a` or `--aggregator`: Type of aggregator function to use. One of `max`, `min`, `mean`, `median` (default: `mean`).
- `-g` or `--greyscale`: If this flag is set, the image is converted to greyscale before downscaling (reduces the number of output features by factor 3).
- `-b` or `--block_size`: Size of one block that will be reduced to a single number. Defaults to 1.

#### 3.2.3 Cluster Analysis of Feature Vectors
The point of data set augmentation is to create a larger variety of input images and to introduce some additional noise into the data set. The script `cluster_analysis.py` takes a file of feature vectors and analyzes whether they form strong clusters (in the sense that all augmented images based on the same original are very similar to each other, but very different from other images). It uses the Silhouette coefficient to quantify this. As comparison, the Silhouette coefficient of a shuffled data set is computed. The script can be called as follows:
```
python -m code.ml.regression.cluster_analysis path/to/features.pickle
```
Here, `features.pickle` is the pickle file generated by either `ann_features.py` or `pixel_features.py`. The script takes the following optional arguments:
- `-n` or `--n_sample`: The number of samples to randomly draw for each original image (defaults to 100). Computing the Silhouette coefficient may be untractable for large data sets, so sampling might be required.
- `-s` or `--seed`: The random seed to use for initializing the random number generator. If none is given, a different initialization is used in every call to the script.

#### 3.2.4 Regression and Baselines

The script `regression.py` can be used to run a linear regression, a lasso regression, or any of the baselines. It is called as follows:
```
python -m code.ml.regression.regression path/to/target_vectors.pickle space_name path/to/features.pickle path/to/folds.csv path/to/output.csv 
```
Here, `target_vectors.pickle` is the file generated by `prepare_targets.py`, `space_name` is the name of a target space contained in this file, `features.pickle` contains the features to be used (either generated by `ann_features.py` or by `pixel_features.py`), `folds.csv` contains the fold structure (for each original image the number of the fold it belongs to), and `output.csv` is the file in which the results will be stored (the script appends to the file if it already exists).

In order to select the type of regression to be used, one needs to pass *exactly one* of the following flags to the script:
- `--zero`: *Zero baseline*, always predicts the origin of the feature space (i.e., a vector where all entries are zero)
- `--mean`: *Mean baseline*, always predicts the mean of the target vectors seen during training.
- `--normal`: *Normal distribution baseline*, estimates a multivariate normal distribution on the target vectors seen during training. Draws a random sample from this distribution for making predictions.
- `--draw`: *Random draw baseline*, uses randomly selected target vectors from the training set for making predictions.
- `--linear`: *Linear regression*, runs sklearn's `LinearRegression` after normalizing the feature space.
- `--lasso`: *Lasso regression, runs sklearn's `Lasso` regressor after normalizing the feature space, using the given value as relative strength of the regularization term. Computes `alpha = args.lasso / len(train_features[0])` to ensure that the regularization term is in the same order of magnitude independent of the size of the feature space.
- `--random_forest`: *Random Forest regression* using a random forest with default parameters as given by sklearn.

In addition to this, the script accepts the following optional parameters:
- `-s` or `--seed`: The random seed to use for initializing the random number generator (important for nondeterministic regressors). If none is given, a different initialization is used in every call to the script.
- `--shuffled`: If this flag is set, the regression is not only performed on the correct targets, but also on the shuffled ones.

The script performs a cross-validation based on the fold structure given in `folds.csv`, where all augmented images that are based on the same original image belong into the same fold. The script reports MSE, MED (the mean Euclidean distance between the predicted points and the targets points), and the coefficient of determination R² in the output csv file for both the training and the test phase.

